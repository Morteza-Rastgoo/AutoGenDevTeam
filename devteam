#!/bin/bash

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
CONFIG_DIR="$SCRIPT_DIR/config"
WORKSPACE_DIR="$SCRIPT_DIR/workspace"
TUNNEL_PID_FILE="$CONFIG_DIR/.tunnel.pid"
OLLAMA_PID_FILE="$CONFIG_DIR/.ollama.pid"
OLLAMA_MODE_FILE="$CONFIG_DIR/.ollama_mode"

# Load environment variables
load_env() {
    if [ -f "$SCRIPT_DIR/.env" ]; then
        set -a
        source "$SCRIPT_DIR/.env"
        set +a
    fi

    # Set defaults if not provided
    OLLAMA_MODEL=${OLLAMA_MODEL:-"deepseek-coder"}
    OLLAMA_LOCAL_PORT=${OLLAMA_LOCAL_PORT:-"11434"}
    
    export PYTHONPATH="$SCRIPT_DIR:$PYTHONPATH"
}

# Get current Ollama mode
get_ollama_mode() {
    if [ -f "$OLLAMA_MODE_FILE" ]; then
        cat "$OLLAMA_MODE_FILE"
    else
        echo ""
    fi
}

# Set Ollama mode
set_ollama_mode() {
    local mode="$1"
    echo "$mode" > "$OLLAMA_MODE_FILE"
}

# Select Ollama mode
select_ollama_mode() {
    local current_mode=$(get_ollama_mode)
    
    if [ -z "$1" ] && [ ! -z "$current_mode" ]; then
        echo "Current Ollama mode: $current_mode"
        return 0
    fi
    
    if [ ! -z "$1" ]; then
        case "$1" in
            "local"|"remote")
                set_ollama_mode "$1"
                echo "Ollama mode set to: $1"
                return 0
                ;;
            *)
                echo "Invalid mode. Please choose 'local' or 'remote'"
                return 1
                ;;
        esac
    fi
    
    echo "Please select Ollama mode:"
    echo "1) Local installation"
    echo "2) Remote server"
    read -p "Enter choice [1-2]: " choice
    
    case "$choice" in
        1)
            if ! command -v ollama >/dev/null 2>&1; then
                echo "Error: Local Ollama is not installed. Please install it first."
                echo "Visit: https://ollama.com/download"
                return 1
            fi
            set_ollama_mode "local"
            echo "Using local Ollama installation"
            ;;
        2)
            if ! check_remote_config; then
                check_remote_config "show"
                return 1
            fi
            set_ollama_mode "remote"
            echo "Using remote Ollama server"
            ;;
        *)
            echo "Invalid choice"
            return 1
            ;;
    esac
    return 0
}

# Ensure virtual environment exists and is activated
setup_env() {
    mkdir -p "$CONFIG_DIR" "$WORKSPACE_DIR"
    
    # Remove existing venv if installation failed previously
    if [ -f "$SCRIPT_DIR/venv/setup_failed" ]; then
        echo "Previous setup failed. Removing old virtual environment..."
        rm -rf "$SCRIPT_DIR/venv"
    fi
    
    if [ ! -d "$SCRIPT_DIR/venv" ]; then
        echo "Setting up Python environment..."
        python3 -m venv "$SCRIPT_DIR/venv"
        if [ $? -ne 0 ]; then
            echo "Error: Failed to create virtual environment"
            exit 1
        fi
        
        # Create marker file
        touch "$SCRIPT_DIR/venv/setup_failed"
        
        source "$SCRIPT_DIR/venv/bin/activate"
        
        echo "Upgrading pip..."
        python3 -m pip install --upgrade pip wheel setuptools >/dev/null 2>&1
        
        echo "Installing Python packages..."
        if python3 -m pip install -r "$CONFIG_DIR/requirements.txt"; then
            rm -f "$SCRIPT_DIR/venv/setup_failed"
            echo "Package installation successful!"
        else
            echo "Error: Package installation failed"
            exit 1
        fi
    else
        source "$SCRIPT_DIR/venv/bin/activate"
        
        # Check if packages need to be updated
        if ! python3 - <<'END'
import pkg_resources
import sys
required = {}
with open("config/requirements.txt") as f:
    for line in f:
        line = line.strip()
        if line and not line.startswith("#"):
            if ">=" in line:
                pkg, ver = line.split(">=")
                required[pkg.strip()] = ver.strip()
            elif "==" in line:
                pkg, ver = line.split("==")
                required[pkg.strip()] = ver.strip()

needs_update = False
for pkg, ver in required.items():
    try:
        installed = pkg_resources.get_distribution(pkg)
        if ver and installed.version != ver and ">=" not in line:
            print(f"Package {pkg} version mismatch: installed={installed.version}, required={ver}")
            needs_update = True
            break
    except pkg_resources.DistributionNotFound:
        print(f"Package {pkg} not found")
        needs_update = True
        break

sys.exit(1 if needs_update else 0)
END
        then
            echo "Updating Python packages..."
            if ! python3 -m pip install -r "$CONFIG_DIR/requirements.txt"; then
                echo "Error: Package update failed"
                exit 1
            fi
        fi
    fi
    
    # Verify installation
    if ! python3 -c "import crewai" 2>/dev/null; then
        echo "Error: Failed to import crewai. Removing virtual environment..."
        rm -rf "$SCRIPT_DIR/venv"
        echo "Please run setup again"
        exit 1
    fi
    
    echo "Environment setup completed successfully!"
}

# Check if remote configuration is available in environment
check_remote_config() {
    local missing_vars=()
    local required_vars=("OLLAMA_REMOTE_HOST" "OLLAMA_REMOTE_USER" "OLLAMA_SSH_KEY" "OLLAMA_REMOTE_PORT")
    
    for var in "${required_vars[@]}"; do
        if [ -z "${!var}" ]; then
            missing_vars+=("$var")
        fi
    done
    
    if [ ${#missing_vars[@]} -eq 0 ]; then
        return 0
    else
        if [ "$1" = "show" ]; then
            echo "Missing required environment variables:"
            printf '%s\n' "${missing_vars[@]}"
            echo "Please set them in .env file or export them directly"
        fi
        return 1
    fi
}

# Setup SSH tunnel to remote Ollama server
setup_tunnel() {
    if ! check_remote_config; then
        echo "Remote configuration not found. Please set up remote configuration."
        return 1
    fi
    
    # Kill any existing tunnels
    if [ -f "$TUNNEL_PID_FILE" ]; then
        OLD_PID=$(cat "$TUNNEL_PID_FILE")
        if kill -0 "$OLD_PID" 2>/dev/null; then
            echo "Stopping existing tunnel..."
            kill "$OLD_PID"
            rm "$TUNNEL_PID_FILE"
            sleep 2
        fi
    fi

    # Kill any process using our local port
    local pid=$(lsof -ti:${OLLAMA_LOCAL_PORT})
    if [ ! -z "$pid" ]; then
        echo "Port ${OLLAMA_LOCAL_PORT} is in use. Freeing it..."
        kill -9 "$pid"
        sleep 2
    fi

    echo "Setting up tunnel to remote Ollama server..."
    ssh -f -N -L "${OLLAMA_LOCAL_PORT}:localhost:${OLLAMA_REMOTE_PORT}" \
        "${OLLAMA_REMOTE_USER}@${OLLAMA_REMOTE_HOST}" \
        -i "${OLLAMA_SSH_KEY}" \
        -o StrictHostKeyChecking=accept-new \
        -o ExitOnForwardFailure=yes
    
    if [ $? -eq 0 ]; then
        echo $! > "$TUNNEL_PID_FILE"
        
        # Wait for tunnel to be ready
        echo "Waiting for tunnel to be ready..."
        for i in {1..10}; do
            if nc -z localhost "${OLLAMA_LOCAL_PORT}" 2>/dev/null; then
        echo "Tunnel established successfully"
                return 0
            fi
            sleep 1
        done
        echo "Warning: Tunnel established but connection not verified"
        return 0
    else
        echo "Failed to establish tunnel"
        return 1
    fi
}

# Stop SSH tunnel
stop_tunnel() {
    if [ -f "$TUNNEL_PID_FILE" ]; then
        PID=$(cat "$TUNNEL_PID_FILE")
        if kill -0 "$PID" 2>/dev/null; then
            kill "$PID"
            rm "$TUNNEL_PID_FILE"
            echo "Tunnel stopped"
        fi
    fi
}

# Test Ollama connection
test_ollama_connection() {
    local url="$1"
    local max_retries=5
    local retry=0
    
    while [ $retry -lt $max_retries ]; do
        if curl -s "$url/api/version" >/dev/null 2>&1; then
            return 0
        fi
        retry=$((retry + 1))
        sleep 1
    done
    return 1
}

# Optimize Ollama configuration for maximum performance
optimize_ollama() {
    if ! check_remote_config; then
        echo "Remote configuration not found. Skipping optimization."
        return 1
    fi

    echo "Optimizing Ollama configuration..."
    
    # Get hardware information
    local cpu_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "lscpu")
    local mem_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "free -g")
    local gpu_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "nvidia-smi -L")
    
    # Extract CPU threads
    local cpu_threads=$(echo "$cpu_info" | grep "^CPU(s):" | awk '{print $2}')
    # Use 95% of available threads
    local num_threads=$((cpu_threads * 95 / 100))
    
    # Extract total memory in GB
    local total_mem=$(echo "$mem_info" | awk '/^Mem:/ {print $2}')
    # Calculate context size based on available memory (75% of total)
    local ctx_size=$((total_mem * 1024 * 3 / 4))
    
    # Check if GPU is available
    local gpu_config=""
    if [ ! -z "$gpu_info" ]; then
        # Get GPU memory in MB
        local gpu_mem=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits")
        if [ ! -z "$gpu_mem" ]; then
            # Use 95% of GPU memory
            local cache_size=$((gpu_mem * 95 / 100))
            gpu_config='"gpu_layers": 43, "num_gpu": 1, "gpu_memory_utilization": 0.95, "cache_capacity_mb": '$cache_size','
            
            # Set GPU to exclusive process mode
            ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo nvidia-smi -c EXCLUSIVE_PROCESS" >/dev/null 2>&1
        fi
    fi
    
    # Create optimized configuration
    local config='{
        '$gpu_config'
        "num_thread": '$num_threads',
        "num_batch": 512,
        "num_ctx": '$ctx_size',
        "num_keep": '$((num_threads * 2))',
        "num_gqa": 8,
        "f16": true,
        "mmap": true,
        "rope_frequency_base": 10000,
        "rope_frequency_scale": 1.0
    }'
    
    # Apply configuration
    echo "Applying optimized configuration..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo mkdir -p /etc/ollama && echo '$config' | sudo tee /etc/ollama/config.json >/dev/null"
    
    # Restart Ollama service
    echo "Restarting Ollama service..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo systemctl restart ollama"
    
    # Set process priority
    echo "Setting process priority..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo renice -n -19 \$(pgrep ollama) && sudo chrt -f -p 99 \$(pgrep ollama)" >/dev/null 2>&1
    
    echo "Optimization completed successfully!"
    return 0
}

# Ensure Ollama is running with required model
ensure_ollama() {
    local mode=$(get_ollama_mode)
    
    if [ -z "$mode" ]; then
        echo "Ollama mode not selected."
        if ! select_ollama_mode; then
            echo "Failed to select Ollama mode"
            exit 1
        fi
        mode=$(get_ollama_mode)
    fi
    
    case "$mode" in
        "local")
            setup_local_ollama
            export OLLAMA_BASE_URL="http://localhost:11434"
            ;;
        "remote")
            local base_url="$OLLAMA_BASE_URL"
            echo "Checking connection to remote Ollama server..."
            
            # Test connection
            if ! curl -s "$base_url/version" >/dev/null; then
                echo "Error: Could not connect to Ollama server"
                echo "Checking if Ollama is running on remote host..."
                
                # Try to start Ollama on remote host
                ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama serve >/dev/null 2>&1 &"
                sleep 5
                
                # Test connection again
                if ! curl -s "$base_url/version" >/dev/null; then
                    echo "Error: Failed to connect to Ollama server"
        exit 1
                fi
            fi
            
            # Check if model exists on remote
            echo "Checking if model $OLLAMA_MODEL exists on remote..."
            if ! ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama list" | grep -q "$OLLAMA_MODEL"; then
                echo "Pulling $OLLAMA_MODEL on remote host..."
                ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama pull $OLLAMA_MODEL"
    fi
    
    export OLLAMA_BASE_URL="$base_url"
            ;;
        *)
            echo "Invalid Ollama mode: $mode"
            exit 1
            ;;
    esac
}

# Setup local Ollama instance
setup_local_ollama() {
    if ! command -v ollama >/dev/null 2>&1; then
        echo "Error: Ollama is not installed. Please install it first."
        echo "Visit: https://ollama.com/download"
        exit 1
    fi

    if ! pgrep -x "ollama" > /dev/null; then
        echo "Starting Ollama server..."
        ollama serve >/dev/null 2>&1 &
        echo $! > "$OLLAMA_PID_FILE"
        sleep 3
    fi
    
    echo "Checking Ollama model..."
    if ! ollama list 2>/dev/null | grep -q "${OLLAMA_MODEL}"; then
        echo "Pulling ${OLLAMA_MODEL} model..."
        if ! ollama pull "${OLLAMA_MODEL}" >/dev/null 2>&1; then
            echo "Error: Failed to pull ${OLLAMA_MODEL} model"
            exit 1
        fi
    fi

    # Apply optimized settings for deepseek-coder
    if [ "${OLLAMA_MODEL}" = "deepseek-coder" ]; then
        echo "Applying optimized settings for deepseek-coder..."
        cat > /tmp/modelfile <<EOF
FROM deepseek-coder

# Optimize for coding tasks
PARAMETER temperature 0.3
PARAMETER top_p 0.95
PARAMETER top_k 40
PARAMETER num_ctx 16384
PARAMETER stop "<|endoftext|>"
PARAMETER stop "```"

# System prompt optimization
SYSTEM """You are an expert software developer with deep knowledge of multiple programming languages, frameworks, and best practices.
Focus on writing clean, efficient, and well-documented code.
Always include proper error handling, input validation, and resource cleanup.
Follow language-specific conventions and best practices.
When reviewing code, be thorough and provide constructive feedback."""
EOF
        ollama create deepseek-coder-optimized /tmp/modelfile
        rm /tmp/modelfile
        OLLAMA_MODEL="deepseek-coder-optimized"
    fi
}

# Main team interaction script
run_team() {
    local task_msg="$1"
    python3 - "$task_msg" << 'EOF' | tee -a "$SCRIPT_DIR/devteam.log"
#!/usr/bin/env python3

import os
import sys
import json
import time
import signal
import requests
import subprocess
from typing import List, Optional, Dict, Any, Callable
from pydantic import BaseModel, Field
from langchain.tools import BaseTool, Tool
from crewai import Agent, Task, Crew, Process
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_ollama import ChatOllama
from termcolor import colored

# Constants
WORKSPACE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "workspace")
os.makedirs(WORKSPACE_DIR, exist_ok=True)

# Define tool schemas
class WriteFileInput(BaseModel):
    filename: str = Field(description="Name of the file to write")
    content: str = Field(description="Content to write to the file")
    
    model_config = {"extra": "forbid"}

class ReadFileInput(BaseModel):
    filename: str = Field(description="Name of the file to read")
    
    model_config = {"extra": "forbid"}

def write_file(filename: str, content: str) -> str:
    """Write content to a file in the workspace directory."""
    try:
        file_path = os.path.join(WORKSPACE_DIR, filename)
        with open(file_path, 'w') as f:
            f.write(content)
        return f"Successfully wrote to {filename}"
    except Exception as e:
        return f"Error writing to file: {str(e)}"

def read_file(filename: str) -> str:
    """Read content from a file in the workspace directory."""
    try:
        file_path = os.path.join(WORKSPACE_DIR, filename)
        with open(file_path, 'r') as f:
            return f.read()
    except Exception as e:
        return f"Error reading file: {str(e)}"

def list_files(tool_input: str = "") -> str:
    """List all files in the workspace directory."""
    try:
        files = os.listdir(WORKSPACE_DIR)
        return "\n".join(files) if files else "No files found in workspace"
    except Exception as e:
        return f"Error listing files: {str(e)}"

# Create tools list
tools = [
    Tool(
        name="write_file",
        func=write_file,
        description="Write content to a file in the workspace directory",
        args_schema=WriteFileInput
    ),
    Tool(
        name="read_file",
        func=read_file,
        description="Read content from a file in the workspace directory",
        args_schema=ReadFileInput
    ),
    Tool(
        name="list_files",
        func=list_files,
        description="List all files in the workspace directory"
    )
]

# Define agent and task models
class AgentConfig(BaseModel):
    role: str = Field(description="The role of the agent")
    goal: str = Field(description="The goal or objective of the agent")
    backstory: str = Field(description="The background story and expertise of the agent")
    verbose: bool = Field(default=True, description="Whether to enable verbose output")
    allow_delegation: bool = Field(default=False, description="Whether to allow task delegation")
    
    model_config = {"extra": "forbid"}

class TaskConfig(BaseModel):
    description: str = Field(description="The task description")
    agent_role: str = Field(description="The role of the agent assigned to this task")
    context: Optional[Dict[str, Any]] = Field(default=None, description="Additional context for the task")
    
    model_config = {"extra": "forbid"}

def create_task_configs(task_description: str) -> List[TaskConfig]:
    """Create dynamic task configurations based on the user's task description."""
    return [
        TaskConfig(
            description=f"""Implement the following task: {task_description}

            Requirements:
            1. Analyze the requirements and design an optimal solution
            2. Create necessary files with proper structure and organization
            3. Implement the solution with clean, efficient code
            4. Include comprehensive error handling and resource cleanup
            5. Add detailed comments and documentation
            6. Follow language-specific best practices and conventions
            
            Available tools:
            - write_file: Create or update files (parameters: filename, content)
            - read_file: Read file contents (parameters: filename)
            - list_files: List workspace files (no parameters)
            
            Workspace directory: {WORKSPACE_DIR}
            
            Guidelines:
            - Write modular and reusable code
            - Include input validation and error messages
            - Use appropriate data structures and algorithms
            - Consider performance and resource usage
            - Add type hints and docstrings where applicable""",
            agent_role='Software Developer'
        ),
        TaskConfig(
            description=f"""Review and verify the implementation for: {task_description}
            
            Requirements:
            1. Perform a comprehensive code review
            2. Verify code quality, efficiency, and best practices
            3. Check error handling, edge cases, and resource management
            4. Validate documentation and code comments
            5. Assess code organization and structure
            6. Write a detailed review report
            
            Available tools:
            - read_file: Read file contents (parameters: filename)
            - write_file: Write review report (parameters: filename, content)
            - list_files: List workspace files (no parameters)
            
            Review checklist:
            1. Code correctness and functionality
            2. Performance and optimization
            3. Error handling and edge cases
            4. Documentation and comments
            5. Best practices and conventions
            6. Security considerations
            7. Resource management
            8. Code organization
            
            Write findings in 'code_review.txt' with:
            - Overview of reviewed files
            - Detailed analysis of each component
            - Identified issues or concerns
            - Suggestions for improvements
            - Best practices compliance
            - Overall assessment""",
            agent_role='Code Reviewer'
        )
    ]

def wait_for_ollama(base_url: str, timeout: int = 30, interval: int = 2) -> bool:
    """Wait for Ollama server to be ready"""
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(f"{base_url}/api/version")
            if response.status_code == 200:
                return True
        except requests.exceptions.RequestException:
            pass
        time.sleep(interval)
    return False

# Get task message from command line argument
task_description = sys.argv[1]

# Setup Ollama configuration
base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
model = os.getenv("OLLAMA_MODEL", "llama2")

# Wait for Ollama to be ready
print(colored("\nChecking Ollama connection...", "yellow"))
if not wait_for_ollama(base_url):
    print(colored("Error: Could not connect to Ollama server", "red"))
    sys.exit(1)

# Initialize Ollama LLM with streaming
llm = ChatOllama(
    base_url=base_url,
    model=model,
    callbacks=[StreamingStdOutCallbackHandler()],
    temperature=0.3,  # Lower temperature for more focused code generation
    timeout=120,
    num_ctx=16384,  # Larger context window for code
    top_p=0.95,
    top_k=40,
    repeat_penalty=1.1,  # Reduce repetition in generated code
    num_predict=2048  # Increase max tokens for longer code generation
)

# Define agent configurations
agent_configs = [
    AgentConfig(
        role='Software Developer',
        goal='Implement high-quality code with optimal solutions',
        backstory="""Expert software developer specializing in efficient and maintainable code.
        Deep knowledge of software design patterns, algorithms, and data structures.
        Experienced in multiple programming languages and modern development practices.
        Strong focus on code optimization, security, and scalability.
        Always implements comprehensive error handling and proper resource management."""
    ),
    AgentConfig(
        role='Code Reviewer',
        goal='Ensure code excellence and best practices',
        backstory="""Senior code reviewer with extensive experience in code quality assurance.
        Expert in identifying potential bugs, security vulnerabilities, and performance issues.
        Deep understanding of clean code principles and software architecture.
        Focuses on code maintainability, readability, and adherence to best practices.
        Provides detailed feedback for code improvements and optimization opportunities."""
    )
]

# Create agents
agents = []
for config in agent_configs:
    agent = Agent(
        role=config.role,
        goal=config.goal,
        backstory=config.backstory,
        verbose=config.verbose,
        allow_delegation=config.allow_delegation,
        llm=llm,
        tools=tools
    )
    agents.append(agent)

# Create dynamic tasks based on user's description
task_configs = create_task_configs(task_description)

# Create tasks
tasks = []
for config in task_configs:
    agent = next((a for a in agents if a.role == config.agent_role), None)
    if agent:
        task = Task(
            description=config.description,
            expected_output="A detailed report of the completed task including any files created or modified.",
            agent=agent,
            context={
                "workspace_dir": WORKSPACE_DIR,
                "tools_available": [
                    "write_file: Write content to files",
                    "read_file: Read content from files",
                    "list_files: List workspace files"
                ]
            }
        )
        tasks.append(task)

# Create and run crew
crew = Crew(
    agents=agents,
    tasks=tasks,
    verbose=2,
    process=Process.sequential
)

# Execute the task
result = crew.kickoff()
print("\nTask execution completed!")
print("Result:", result)

EOF
}

# Cleanup function
cleanup() {
    stop_tunnel
    if [ -f "$OLLAMA_PID_FILE" ]; then
        PID=$(cat "$OLLAMA_PID_FILE")
        if kill -0 "$PID" 2>/dev/null; then
            kill "$PID"
            rm "$OLLAMA_PID_FILE"
            echo "Ollama server stopped"
        fi
    fi
    exit 0
}

# Set up trap for cleanup
trap cleanup EXIT INT TERM

# Load environment variables
load_env

# Command line interface
case "$1" in
    "setup")
        setup_env
        if ! select_ollama_mode; then
            echo "Failed to select Ollama mode"
            exit 1
        fi
        ensure_ollama
        echo "Optimizing Ollama for maximum performance..."
        if [ "$(get_ollama_mode)" = "remote" ] && optimize_ollama; then
            echo "Ollama optimization completed."
            touch "$CONFIG_DIR/.ollama_optimized"
        else
            echo "Warning: Ollama optimization skipped or failed, continuing with default settings."
        fi
        echo "DevTeam environment is ready!"
        ;;
    "mode")
        case "$2" in
            "")
                select_ollama_mode
                ;;
            "local"|"remote")
                select_ollama_mode "$2"
                ;;
            *)
                echo "Usage: ./devteam mode [local|remote]"
                exit 1
                ;;
        esac
        ;;
    "task")
        if [ -z "$2" ]; then
            echo "Usage: ./devteam task 'your task description'"
            exit 1
        fi
        setup_env
        ensure_ollama
        run_team "$2"
        ;;
    *)
        echo "Usage:"
        echo "  ./devteam setup              - Set up the development environment"
        echo "  ./devteam mode [local|remote] - Select Ollama mode (local/remote)"
        echo "  ./devteam task 'description' - Assign a task to the team"
        exit 1
        ;;
esac 