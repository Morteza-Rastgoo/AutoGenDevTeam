#!/bin/bash

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
CONFIG_DIR="$SCRIPT_DIR/config"
WORKSPACE_DIR="$SCRIPT_DIR/workspace"
TUNNEL_PID_FILE="$CONFIG_DIR/.tunnel.pid"
OLLAMA_PID_FILE="$CONFIG_DIR/.ollama.pid"

# Load environment variables
load_env() {
    if [ -f "$SCRIPT_DIR/.env" ]; then
        set -a
        source "$SCRIPT_DIR/.env"
        set +a
    fi

    # Set defaults if not provided
    OLLAMA_MODEL=${OLLAMA_MODEL:-"llama2"}
    OLLAMA_LOCAL_PORT=${OLLAMA_LOCAL_PORT:-"11434"}
    
    export PYTHONPATH="$SCRIPT_DIR:$PYTHONPATH"
}

# Ensure virtual environment exists and is activated
setup_env() {
    mkdir -p "$CONFIG_DIR" "$WORKSPACE_DIR"
    
    # Remove existing venv if installation failed previously
    if [ -f "$SCRIPT_DIR/venv/setup_failed" ]; then
        echo "Previous setup failed. Removing old virtual environment..."
        rm -rf "$SCRIPT_DIR/venv"
    fi
    
    if [ ! -d "$SCRIPT_DIR/venv" ]; then
        echo "Setting up Python environment..."
        python3 -m venv "$SCRIPT_DIR/venv"
        if [ $? -ne 0 ]; then
            echo "Error: Failed to create virtual environment"
            exit 1
        fi
        
        # Create marker file
        touch "$SCRIPT_DIR/venv/setup_failed"
        
        source "$SCRIPT_DIR/venv/bin/activate"
        
        echo "Upgrading pip..."
        python3 -m pip install --upgrade pip wheel setuptools >/dev/null 2>&1
        
        echo "Installing Python packages..."
        if python3 -m pip install -r "$CONFIG_DIR/requirements.txt"; then
            rm -f "$SCRIPT_DIR/venv/setup_failed"
            echo "Package installation successful!"
        else
            echo "Error: Package installation failed"
            exit 1
        fi
    else
        source "$SCRIPT_DIR/venv/bin/activate"
        
        # Check if packages need to be updated
        if ! python3 - <<'END'
import pkg_resources
import sys
required = {}
with open("config/requirements.txt") as f:
    for line in f:
        line = line.strip()
        if line and not line.startswith("#"):
            if ">=" in line:
                pkg, ver = line.split(">=")
                required[pkg.strip()] = ver.strip()
            elif "==" in line:
                pkg, ver = line.split("==")
                required[pkg.strip()] = ver.strip()

needs_update = False
for pkg, ver in required.items():
    try:
        installed = pkg_resources.get_distribution(pkg)
        if ver and installed.version != ver and ">=" not in line:
            print(f"Package {pkg} version mismatch: installed={installed.version}, required={ver}")
            needs_update = True
            break
    except pkg_resources.DistributionNotFound:
        print(f"Package {pkg} not found")
        needs_update = True
        break

sys.exit(1 if needs_update else 0)
END
        then
            echo "Updating Python packages..."
            if ! python3 -m pip install -r "$CONFIG_DIR/requirements.txt"; then
                echo "Error: Package update failed"
                exit 1
            fi
        fi
    fi
    
    # Verify installation
    if ! python3 -c "import crewai" 2>/dev/null; then
        echo "Error: Failed to import crewai. Removing virtual environment..."
        rm -rf "$SCRIPT_DIR/venv"
        echo "Please run setup again"
        exit 1
    fi
    
    echo "Environment setup completed successfully!"
}

# Check if remote configuration is available in environment
check_remote_config() {
    local missing_vars=()
    local required_vars=("OLLAMA_REMOTE_HOST" "OLLAMA_REMOTE_USER" "OLLAMA_SSH_KEY" "OLLAMA_REMOTE_PORT")
    
    for var in "${required_vars[@]}"; do
        if [ -z "${!var}" ]; then
            missing_vars+=("$var")
        fi
    done
    
    if [ ${#missing_vars[@]} -eq 0 ]; then
        return 0
    else
        if [ "$1" = "show" ]; then
            echo "Missing required environment variables:"
            printf '%s\n' "${missing_vars[@]}"
            echo "Please set them in .env file or export them directly"
        fi
        return 1
    fi
}

# Setup SSH tunnel to remote Ollama server
setup_tunnel() {
    if ! check_remote_config; then
        echo "Remote configuration not found. Please set up remote configuration."
        return 1
    fi
    
    # Kill any existing tunnels
    if [ -f "$TUNNEL_PID_FILE" ]; then
        OLD_PID=$(cat "$TUNNEL_PID_FILE")
        if kill -0 "$OLD_PID" 2>/dev/null; then
            echo "Stopping existing tunnel..."
            kill "$OLD_PID"
            rm "$TUNNEL_PID_FILE"
            sleep 2
        fi
    fi

    # Kill any process using our local port
    local pid=$(lsof -ti:${OLLAMA_LOCAL_PORT})
    if [ ! -z "$pid" ]; then
        echo "Port ${OLLAMA_LOCAL_PORT} is in use. Freeing it..."
        kill -9 "$pid"
        sleep 2
    fi

    echo "Setting up tunnel to remote Ollama server..."
    ssh -f -N -L "${OLLAMA_LOCAL_PORT}:localhost:${OLLAMA_REMOTE_PORT}" \
        "${OLLAMA_REMOTE_USER}@${OLLAMA_REMOTE_HOST}" \
        -i "${OLLAMA_SSH_KEY}" \
        -o StrictHostKeyChecking=accept-new \
        -o ExitOnForwardFailure=yes
    
    if [ $? -eq 0 ]; then
        echo $! > "$TUNNEL_PID_FILE"
        
        # Wait for tunnel to be ready
        echo "Waiting for tunnel to be ready..."
        for i in {1..10}; do
            if nc -z localhost "${OLLAMA_LOCAL_PORT}" 2>/dev/null; then
                echo "Tunnel established successfully"
                return 0
            fi
            sleep 1
        done
        echo "Warning: Tunnel established but connection not verified"
        return 0
    else
        echo "Failed to establish tunnel"
        return 1
    fi
}

# Stop SSH tunnel
stop_tunnel() {
    if [ -f "$TUNNEL_PID_FILE" ]; then
        PID=$(cat "$TUNNEL_PID_FILE")
        if kill -0 "$PID" 2>/dev/null; then
            kill "$PID"
            rm "$TUNNEL_PID_FILE"
            echo "Tunnel stopped"
        fi
    fi
}

# Test Ollama connection
test_ollama_connection() {
    local url="$1"
    local max_retries=5
    local retry=0
    
    while [ $retry -lt $max_retries ]; do
        if curl -s "$url/api/version" >/dev/null 2>&1; then
            return 0
        fi
        retry=$((retry + 1))
        sleep 1
    done
    return 1
}

# Optimize Ollama configuration for maximum performance
optimize_ollama() {
    if ! check_remote_config; then
        echo "Remote configuration not found. Skipping optimization."
        return 1
    fi

    echo "Optimizing Ollama configuration..."
    
    # Get hardware information
    local cpu_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "lscpu")
    local mem_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "free -g")
    local gpu_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "nvidia-smi -L")
    
    # Extract CPU threads
    local cpu_threads=$(echo "$cpu_info" | grep "^CPU(s):" | awk '{print $2}')
    # Use 95% of available threads
    local num_threads=$((cpu_threads * 95 / 100))
    
    # Extract total memory in GB
    local total_mem=$(echo "$mem_info" | awk '/^Mem:/ {print $2}')
    # Calculate context size based on available memory (75% of total)
    local ctx_size=$((total_mem * 1024 * 3 / 4))
    
    # Check if GPU is available
    local gpu_config=""
    if [ ! -z "$gpu_info" ]; then
        # Get GPU memory in MB
        local gpu_mem=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits")
        if [ ! -z "$gpu_mem" ]; then
            # Use 95% of GPU memory
            local cache_size=$((gpu_mem * 95 / 100))
            gpu_config='"gpu_layers": 43, "num_gpu": 1, "gpu_memory_utilization": 0.95, "cache_capacity_mb": '$cache_size','
            
            # Set GPU to exclusive process mode
            ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo nvidia-smi -c EXCLUSIVE_PROCESS" >/dev/null 2>&1
        fi
    fi
    
    # Create optimized configuration
    local config='{
        '$gpu_config'
        "num_thread": '$num_threads',
        "num_batch": 512,
        "num_ctx": '$ctx_size',
        "num_keep": '$((num_threads * 2))',
        "num_gqa": 8,
        "f16": true,
        "mmap": true,
        "rope_frequency_base": 10000,
        "rope_frequency_scale": 1.0
    }'
    
    # Apply configuration
    echo "Applying optimized configuration..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo mkdir -p /etc/ollama && echo '$config' | sudo tee /etc/ollama/config.json >/dev/null"
    
    # Restart Ollama service
    echo "Restarting Ollama service..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo systemctl restart ollama"
    
    # Set process priority
    echo "Setting process priority..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo renice -n -19 \$(pgrep ollama) && sudo chrt -f -p 99 \$(pgrep ollama)" >/dev/null 2>&1
    
    echo "Optimization completed successfully!"
    return 0
}

# Ensure Ollama is running with required model
ensure_ollama() {
    local base_url="$OLLAMA_BASE_URL"
    
    if check_remote_config; then
        echo "Checking connection to remote Ollama server..."
        
        # Optimize Ollama configuration
        optimize_ollama
        
        # Test connection
        if ! curl -s "$base_url/version" >/dev/null; then
            echo "Error: Could not connect to Ollama server"
            echo "Checking if Ollama is running on remote host..."
            
            # Try to start Ollama on remote host
            ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama serve >/dev/null 2>&1 &"
            sleep 5
            
            # Test connection again
            if ! curl -s "$base_url/version" >/dev/null; then
                echo "Error: Failed to connect to Ollama server"
                exit 1
            fi
        fi
        
        # Check if model exists on remote
        echo "Checking if model $OLLAMA_MODEL exists on remote..."
        if ! ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama list" | grep -q "$OLLAMA_MODEL"; then
            echo "Pulling $OLLAMA_MODEL on remote host..."
            ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama pull $OLLAMA_MODEL"
        fi
        
        export OLLAMA_BASE_URL="$base_url"
    else
        echo "Error: Remote configuration not found"
        echo "Please set up remote configuration in .env file"
        exit 1
    fi
}

# Setup local Ollama instance
setup_local_ollama() {
    if ! command -v ollama >/dev/null 2>&1; then
        echo "Error: Ollama is not installed. Please install it first."
        echo "Visit: https://ollama.com/download"
        exit 1
    fi

    if ! pgrep -x "ollama" > /dev/null; then
        echo "Starting Ollama server..."
        ollama serve >/dev/null 2>&1 &
        echo $! > "$OLLAMA_PID_FILE"
        sleep 3
    fi
    
    echo "Checking Ollama model..."
    if ! ollama list 2>/dev/null | grep -q "${OLLAMA_MODEL}"; then
        echo "Pulling ${OLLAMA_MODEL} model..."
        if ! ollama pull "${OLLAMA_MODEL}" >/dev/null 2>&1; then
            echo "Error: Failed to pull ${OLLAMA_MODEL} model"
            exit 1
        fi
    fi
}

# Main team interaction script
run_team() {
    local task_msg="$1"
    python3 - "$task_msg" << 'EOF' | tee -a "$SCRIPT_DIR/devteam.log"
from crewai import Agent, Task, Crew, Process
from langchain_community.llms import Ollama
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import os
from termcolor import colored
import sys
import time
import requests

def wait_for_ollama(base_url, timeout=30, interval=2):
    """Wait for Ollama server to be ready"""
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(f"{base_url}/api/version")
            if response.status_code == 200:
                return True
        except requests.exceptions.RequestException:
            pass
        time.sleep(interval)
    return False

# Get task message from command line argument
task_description = sys.argv[1]

# Setup Ollama configuration
base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
model = os.getenv("OLLAMA_MODEL", "llama2")

# Wait for Ollama to be ready
print(colored("\nChecking Ollama connection...", "yellow"))
if not wait_for_ollama(base_url):
    print(colored("Error: Could not connect to Ollama server", "red"))
    sys.exit(1)

# Initialize Ollama LLM with streaming
llm = Ollama(
    base_url=base_url,
    model=model,
    callbacks=[StreamingStdOutCallbackHandler()],
    temperature=0.5,
    timeout=120
)

# Create agents with specific roles
architect = Agent(
    role='Software Architect',
    goal='Design and plan software architecture',
    backstory="""Expert software architect with extensive experience in system design.
    Focuses on creating clean, maintainable, and well-structured applications.
    Proficient in various design patterns and best practices.""",
    llm=llm,
    verbose=True,
    allow_delegation=False
)

developer = Agent(
    role='Software Developer',
    goal='Implement high-quality code',
    backstory="""Senior developer with strong coding skills and attention to detail.
    Writes clean, efficient, and well-documented code.
    Expert in implementing robust error handling and testing.""",
    llm=llm,
    verbose=True,
    allow_delegation=False
)

reviewer = Agent(
    role='Code Reviewer',
    goal='Ensure code quality and best practices',
    backstory="""Experienced code reviewer with a keen eye for detail.
    Ensures code meets quality standards, follows best practices, and is well-tested.
    Expert in identifying potential issues and suggesting improvements.""",
    llm=llm,
    verbose=True,
    allow_delegation=False
)

# Create tasks with clear instructions
design_task = Task(
    description=f"""Design the architecture for: {task_description}
    
    Requirements:
    1. Create a clear and organized project structure
    2. Define all necessary components and their interactions
    3. Specify required dependencies and their versions
    4. Consider scalability and maintainability
    5. Document design decisions and rationale
    
    Output should include:
    - Project structure
    - Component diagram or description
    - List of dependencies
    - Implementation guidelines""",
    agent=architect
)

implement_task = Task(
    description=f"""Implement the solution based on the architect's design: {task_description}
    
    Requirements:
    1. Follow the provided design architecture
    2. Write clean, well-documented code
    3. Implement proper error handling
    4. Add necessary comments and docstrings
    5. Create required project files
    
    Output should include:
    - Complete implementation
    - Documentation
    - Setup instructions""",
    agent=developer
)

review_task = Task(
    description=f"""Review the implementation of: {task_description}
    
    Review criteria:
    1. Code quality and best practices
    2. Error handling completeness
    3. Documentation quality
    4. Test coverage
    5. Project structure
    
    Output should include:
    - Review findings
    - Suggested improvements
    - Final approval or change requests""",
    agent=reviewer
)

# Create crew with sequential process
crew = Crew(
    agents=[architect, developer, reviewer],
    tasks=[design_task, implement_task, review_task],
    verbose=True,
    process=Process.sequential
)

print(colored("\nStarting task execution...", "green"))
print(colored("Task: " + task_description + "\n", "yellow"))

try:
    # Execute the crew's tasks
    result = crew.kickoff()
    print("\nTask Result:")
    print(colored(result, "cyan"))
except Exception as e:
    print(colored(f"\nError during task execution: {str(e)}", "red"))
    sys.exit(1)
EOF
}

# Cleanup function
cleanup() {
    stop_tunnel
    if [ -f "$OLLAMA_PID_FILE" ]; then
        PID=$(cat "$OLLAMA_PID_FILE")
        if kill -0 "$PID" 2>/dev/null; then
            kill "$PID"
            rm "$OLLAMA_PID_FILE"
            echo "Ollama server stopped"
        fi
    fi
    exit 0
}

# Set up trap for cleanup
trap cleanup EXIT INT TERM

# Load environment variables
load_env

# Command line interface
case "$1" in
    "setup")
        setup_env
        ensure_ollama
        echo "Optimizing Ollama for maximum performance..."
        optimize_ollama
        echo "DevTeam environment is ready!"
        ;;
    "remote")
        case "$2" in
            "status")
                if check_remote_config; then
                    echo "Remote configuration is set:"
                    echo "Host: $OLLAMA_REMOTE_HOST"
                    echo "User: $OLLAMA_REMOTE_USER"
                    echo "Port: $OLLAMA_REMOTE_PORT"
                    echo "SSH Key: $OLLAMA_SSH_KEY"
                    echo "Model: $OLLAMA_MODEL"
                    echo "Local Port: $OLLAMA_LOCAL_PORT"
                else
                    check_remote_config "show"
                fi
                ;;
            "stop")
                stop_tunnel
                ;;
            *)
                echo "Usage:"
                echo "  ./devteam remote status  - Check remote configuration status"
                echo "  ./devteam remote stop    - Stop remote tunnel"
                exit 1
                ;;
        esac
        ;;
    "task")
        if [ -z "$2" ]; then
            echo "Usage: ./devteam task 'your task description'"
            exit 1
        fi
        setup_env
        ensure_ollama
        run_team "$2"
        ;;
    *)
        echo "Usage:"
        echo "  ./devteam setup              - Set up the development environment"
        echo "  ./devteam remote status      - Check remote configuration"
        echo "  ./devteam remote stop        - Stop remote tunnel"
        echo "  ./devteam task 'description' - Assign a task to the team"
        exit 1
        ;;
esac 