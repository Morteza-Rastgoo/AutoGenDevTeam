#!/bin/bash

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
CONFIG_DIR="$SCRIPT_DIR/config"
WORKSPACE_DIR="$SCRIPT_DIR/workspace"
TUNNEL_PID_FILE="$CONFIG_DIR/.tunnel.pid"
OLLAMA_PID_FILE="$CONFIG_DIR/.ollama.pid"
OLLAMA_MODE_FILE="$CONFIG_DIR/.ollama_mode"

# Load environment variables
load_env() {
    if [ -f "$SCRIPT_DIR/.env" ]; then
        set -a
        source "$SCRIPT_DIR/.env"
        set +a
    fi

    # Set defaults if not provided
    OLLAMA_MODEL=${OLLAMA_MODEL:-"llama2"}
    OLLAMA_LOCAL_PORT=${OLLAMA_LOCAL_PORT:-"11434"}
    
    export PYTHONPATH="$SCRIPT_DIR:$PYTHONPATH"
}

# Get current Ollama mode
get_ollama_mode() {
    if [ -f "$OLLAMA_MODE_FILE" ]; then
        cat "$OLLAMA_MODE_FILE"
    else
        echo ""
    fi
}

# Set Ollama mode
set_ollama_mode() {
    local mode="$1"
    echo "$mode" > "$OLLAMA_MODE_FILE"
}

# Select Ollama mode
select_ollama_mode() {
    local current_mode=$(get_ollama_mode)
    
    if [ -z "$1" ] && [ ! -z "$current_mode" ]; then
        echo "Current Ollama mode: $current_mode"
        return 0
    fi
    
    if [ ! -z "$1" ]; then
        case "$1" in
            "local"|"remote")
                set_ollama_mode "$1"
                echo "Ollama mode set to: $1"
                return 0
                ;;
            *)
                echo "Invalid mode. Please choose 'local' or 'remote'"
                return 1
                ;;
        esac
    fi
    
    echo "Please select Ollama mode:"
    echo "1) Local installation"
    echo "2) Remote server"
    read -p "Enter choice [1-2]: " choice
    
    case "$choice" in
        1)
            if ! command -v ollama >/dev/null 2>&1; then
                echo "Error: Local Ollama is not installed. Please install it first."
                echo "Visit: https://ollama.com/download"
                return 1
            fi
            set_ollama_mode "local"
            echo "Using local Ollama installation"
            ;;
        2)
            if ! check_remote_config; then
                check_remote_config "show"
                return 1
            fi
            set_ollama_mode "remote"
            echo "Using remote Ollama server"
            ;;
        *)
            echo "Invalid choice"
            return 1
            ;;
    esac
    return 0
}

# Ensure virtual environment exists and is activated
setup_env() {
    mkdir -p "$CONFIG_DIR" "$WORKSPACE_DIR"
    
    # Remove existing venv if installation failed previously
    if [ -f "$SCRIPT_DIR/venv/setup_failed" ]; then
        echo "Previous setup failed. Removing old virtual environment..."
        rm -rf "$SCRIPT_DIR/venv"
    fi
    
    if [ ! -d "$SCRIPT_DIR/venv" ]; then
        echo "Setting up Python environment..."
        python3 -m venv "$SCRIPT_DIR/venv"
        if [ $? -ne 0 ]; then
            echo "Error: Failed to create virtual environment"
            exit 1
        fi
        
        # Create marker file
        touch "$SCRIPT_DIR/venv/setup_failed"
        
        source "$SCRIPT_DIR/venv/bin/activate"
        
        echo "Upgrading pip..."
        python3 -m pip install --upgrade pip wheel setuptools >/dev/null 2>&1
        
        echo "Installing Python packages..."
        if python3 -m pip install -r "$CONFIG_DIR/requirements.txt"; then
            rm -f "$SCRIPT_DIR/venv/setup_failed"
            echo "Package installation successful!"
        else
            echo "Error: Package installation failed"
            exit 1
        fi
    else
        source "$SCRIPT_DIR/venv/bin/activate"
        
        # Check if packages need to be updated
        if ! python3 - <<'END'
import pkg_resources
import sys
required = {}
with open("config/requirements.txt") as f:
    for line in f:
        line = line.strip()
        if line and not line.startswith("#"):
            if ">=" in line:
                pkg, ver = line.split(">=")
                required[pkg.strip()] = ver.strip()
            elif "==" in line:
                pkg, ver = line.split("==")
                required[pkg.strip()] = ver.strip()

needs_update = False
for pkg, ver in required.items():
    try:
        installed = pkg_resources.get_distribution(pkg)
        if ver and installed.version != ver and ">=" not in line:
            print(f"Package {pkg} version mismatch: installed={installed.version}, required={ver}")
            needs_update = True
            break
    except pkg_resources.DistributionNotFound:
        print(f"Package {pkg} not found")
        needs_update = True
        break

sys.exit(1 if needs_update else 0)
END
        then
            echo "Updating Python packages..."
            if ! python3 -m pip install -r "$CONFIG_DIR/requirements.txt"; then
                echo "Error: Package update failed"
                exit 1
            fi
        fi
    fi
    
    # Verify installation
    if ! python3 -c "import crewai" 2>/dev/null; then
        echo "Error: Failed to import crewai. Removing virtual environment..."
        rm -rf "$SCRIPT_DIR/venv"
        echo "Please run setup again"
        exit 1
    fi
    
    echo "Environment setup completed successfully!"
}

# Check if remote configuration is available in environment
check_remote_config() {
    local missing_vars=()
    local required_vars=("OLLAMA_REMOTE_HOST" "OLLAMA_REMOTE_USER" "OLLAMA_SSH_KEY" "OLLAMA_REMOTE_PORT")
    
    for var in "${required_vars[@]}"; do
        if [ -z "${!var}" ]; then
            missing_vars+=("$var")
        fi
    done
    
    if [ ${#missing_vars[@]} -eq 0 ]; then
        return 0
    else
        if [ "$1" = "show" ]; then
            echo "Missing required environment variables:"
            printf '%s\n' "${missing_vars[@]}"
            echo "Please set them in .env file or export them directly"
        fi
        return 1
    fi
}

# Setup SSH tunnel to remote Ollama server
setup_tunnel() {
    if ! check_remote_config; then
        echo "Remote configuration not found. Please set up remote configuration."
        return 1
    fi
    
    # Kill any existing tunnels
    if [ -f "$TUNNEL_PID_FILE" ]; then
        OLD_PID=$(cat "$TUNNEL_PID_FILE")
        if kill -0 "$OLD_PID" 2>/dev/null; then
            echo "Stopping existing tunnel..."
            kill "$OLD_PID"
            rm "$TUNNEL_PID_FILE"
            sleep 2
        fi
    fi

    # Kill any process using our local port
    local pid=$(lsof -ti:${OLLAMA_LOCAL_PORT})
    if [ ! -z "$pid" ]; then
        echo "Port ${OLLAMA_LOCAL_PORT} is in use. Freeing it..."
        kill -9 "$pid"
        sleep 2
    fi

    echo "Setting up tunnel to remote Ollama server..."
    ssh -f -N -L "${OLLAMA_LOCAL_PORT}:localhost:${OLLAMA_REMOTE_PORT}" \
        "${OLLAMA_REMOTE_USER}@${OLLAMA_REMOTE_HOST}" \
        -i "${OLLAMA_SSH_KEY}" \
        -o StrictHostKeyChecking=accept-new \
        -o ExitOnForwardFailure=yes
    
    if [ $? -eq 0 ]; then
        echo $! > "$TUNNEL_PID_FILE"
        
        # Wait for tunnel to be ready
        echo "Waiting for tunnel to be ready..."
        for i in {1..10}; do
            if nc -z localhost "${OLLAMA_LOCAL_PORT}" 2>/dev/null; then
                echo "Tunnel established successfully"
                return 0
            fi
            sleep 1
        done
        echo "Warning: Tunnel established but connection not verified"
        return 0
    else
        echo "Failed to establish tunnel"
        return 1
    fi
}

# Stop SSH tunnel
stop_tunnel() {
    if [ -f "$TUNNEL_PID_FILE" ]; then
        PID=$(cat "$TUNNEL_PID_FILE")
        if kill -0 "$PID" 2>/dev/null; then
            kill "$PID"
            rm "$TUNNEL_PID_FILE"
            echo "Tunnel stopped"
        fi
    fi
}

# Test Ollama connection
test_ollama_connection() {
    local url="$1"
    local max_retries=5
    local retry=0
    
    while [ $retry -lt $max_retries ]; do
        if curl -s "$url/api/version" >/dev/null 2>&1; then
            return 0
        fi
        retry=$((retry + 1))
        sleep 1
    done
    return 1
}

# Optimize Ollama configuration for maximum performance
optimize_ollama() {
    if ! check_remote_config; then
        echo "Remote configuration not found. Skipping optimization."
        return 1
    fi

    echo "Optimizing Ollama configuration..."
    
    # Get hardware information
    local cpu_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "lscpu")
    local mem_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "free -g")
    local gpu_info=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "nvidia-smi -L")
    
    # Extract CPU threads
    local cpu_threads=$(echo "$cpu_info" | grep "^CPU(s):" | awk '{print $2}')
    # Use 95% of available threads
    local num_threads=$((cpu_threads * 95 / 100))
    
    # Extract total memory in GB
    local total_mem=$(echo "$mem_info" | awk '/^Mem:/ {print $2}')
    # Calculate context size based on available memory (75% of total)
    local ctx_size=$((total_mem * 1024 * 3 / 4))
    
    # Check if GPU is available
    local gpu_config=""
    if [ ! -z "$gpu_info" ]; then
        # Get GPU memory in MB
        local gpu_mem=$(ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits")
        if [ ! -z "$gpu_mem" ]; then
            # Use 95% of GPU memory
            local cache_size=$((gpu_mem * 95 / 100))
            gpu_config='"gpu_layers": 43, "num_gpu": 1, "gpu_memory_utilization": 0.95, "cache_capacity_mb": '$cache_size','
            
            # Set GPU to exclusive process mode
            ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo nvidia-smi -c EXCLUSIVE_PROCESS" >/dev/null 2>&1
        fi
    fi
    
    # Create optimized configuration
    local config='{
        '$gpu_config'
        "num_thread": '$num_threads',
        "num_batch": 512,
        "num_ctx": '$ctx_size',
        "num_keep": '$((num_threads * 2))',
        "num_gqa": 8,
        "f16": true,
        "mmap": true,
        "rope_frequency_base": 10000,
        "rope_frequency_scale": 1.0
    }'
    
    # Apply configuration
    echo "Applying optimized configuration..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo mkdir -p /etc/ollama && echo '$config' | sudo tee /etc/ollama/config.json >/dev/null"
    
    # Restart Ollama service
    echo "Restarting Ollama service..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo systemctl restart ollama"
    
    # Set process priority
    echo "Setting process priority..."
    ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "sudo renice -n -19 \$(pgrep ollama) && sudo chrt -f -p 99 \$(pgrep ollama)" >/dev/null 2>&1
    
    echo "Optimization completed successfully!"
    return 0
}

# Ensure Ollama is running with required model
ensure_ollama() {
    local mode=$(get_ollama_mode)
    
    if [ -z "$mode" ]; then
        echo "Ollama mode not selected."
        if ! select_ollama_mode; then
            echo "Failed to select Ollama mode"
            exit 1
        fi
        mode=$(get_ollama_mode)
    fi
    
    case "$mode" in
        "local")
            setup_local_ollama
            export OLLAMA_BASE_URL="http://localhost:11434"
            ;;
        "remote")
            local base_url="$OLLAMA_BASE_URL"
            echo "Checking connection to remote Ollama server..."
            
            # Test connection
            if ! curl -s "$base_url/version" >/dev/null; then
                echo "Error: Could not connect to Ollama server"
                echo "Checking if Ollama is running on remote host..."
                
                # Try to start Ollama on remote host
                ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama serve >/dev/null 2>&1 &"
                sleep 5
                
                # Test connection again
                if ! curl -s "$base_url/version" >/dev/null; then
                    echo "Error: Failed to connect to Ollama server"
                    exit 1
                fi
            fi
            
            # Check if model exists on remote
            echo "Checking if model $OLLAMA_MODEL exists on remote..."
            if ! ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama list" | grep -q "$OLLAMA_MODEL"; then
                echo "Pulling $OLLAMA_MODEL on remote host..."
                ssh -i "$OLLAMA_SSH_KEY" "$OLLAMA_REMOTE_USER@$OLLAMA_REMOTE_HOST" "ollama pull $OLLAMA_MODEL"
            fi
            
            export OLLAMA_BASE_URL="$base_url"
            ;;
        *)
            echo "Invalid Ollama mode: $mode"
            exit 1
            ;;
    esac
}

# Setup local Ollama instance
setup_local_ollama() {
    if ! command -v ollama >/dev/null 2>&1; then
        echo "Error: Ollama is not installed. Please install it first."
        echo "Visit: https://ollama.com/download"
        exit 1
    fi

    if ! pgrep -x "ollama" > /dev/null; then
        echo "Starting Ollama server..."
        ollama serve >/dev/null 2>&1 &
        echo $! > "$OLLAMA_PID_FILE"
        sleep 3
    fi
    
    echo "Checking Ollama model..."
    if ! ollama list 2>/dev/null | grep -q "${OLLAMA_MODEL}"; then
        echo "Pulling ${OLLAMA_MODEL} model..."
        if ! ollama pull "${OLLAMA_MODEL}" >/dev/null 2>&1; then
            echo "Error: Failed to pull ${OLLAMA_MODEL} model"
            exit 1
        fi
    fi
}

# Main team interaction script
run_team() {
    local task_msg="$1"
    python3 - "$task_msg" << 'EOF' | tee -a "$SCRIPT_DIR/devteam.log"
#!/usr/bin/env python3

import os
import sys
import json
import time
import signal
import requests
import subprocess
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from langchain.tools import BaseTool
from crewai import Agent, Task, Crew, Process
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_community.llms import Ollama
from termcolor import colored

# Constants
WORKSPACE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "workspace")
os.makedirs(WORKSPACE_DIR, exist_ok=True)

# Define tool schemas
class WriteFileInput(BaseModel):
    filename: str = Field(description="Name of the file to write")
    content: str = Field(description="Content to write to the file")
    
    model_config = {"extra": "forbid"}

class ReadFileInput(BaseModel):
    filename: str = Field(description="Name of the file to read")
    
    model_config = {"extra": "forbid"}

# Define file tools
class WriteFileTool(BaseTool):
    name = "write_file"
    description = "Write content to a file in the workspace directory"
    args_schema = WriteFileInput
    
    def _run(self, filename: str, content: str) -> str:
        """Write content to a file in the workspace directory."""
        try:
            file_path = os.path.join(WORKSPACE_DIR, filename)
            with open(file_path, 'w') as f:
                f.write(content)
            return f"Successfully wrote to {filename}"
        except Exception as e:
            return f"Error writing to file: {str(e)}"
            
    def _arun(self, filename: str, content: str) -> str:
        raise NotImplementedError("Async not implemented")

class ReadFileTool(BaseTool):
    name = "read_file"
    description = "Read content from a file in the workspace directory"
    args_schema = ReadFileInput
    
    def _run(self, filename: str) -> str:
        """Read content from a file in the workspace directory."""
        try:
            file_path = os.path.join(WORKSPACE_DIR, filename)
            with open(file_path, 'r') as f:
                return f.read()
        except Exception as e:
            return f"Error reading file: {str(e)}"
            
    def _arun(self, filename: str) -> str:
        raise NotImplementedError("Async not implemented")

class ListFilesTool(BaseTool):
    name = "list_files"
    description = "List all files in the workspace directory"
    
    def _run(self, tool_input: str = "") -> str:
        """List all files in the workspace directory."""
        try:
            files = os.listdir(WORKSPACE_DIR)
            return "\n".join(files) if files else "No files found in workspace"
        except Exception as e:
            return f"Error listing files: {str(e)}"
            
    def _arun(self, tool_input: str = "") -> str:
        raise NotImplementedError("Async not implemented")

# Create tools list
tools = [WriteFileTool(), ReadFileTool(), ListFilesTool()]

# Define agent and task models
class AgentConfig(BaseModel):
    role: str = Field(description="The role of the agent")
    goal: str = Field(description="The goal or objective of the agent")
    backstory: str = Field(description="The background story and expertise of the agent")
    verbose: bool = Field(default=True, description="Whether to enable verbose output")
    allow_delegation: bool = Field(default=False, description="Whether to allow task delegation")
    
    model_config = {"extra": "forbid"}

class TaskConfig(BaseModel):
    description: str = Field(description="The task description")
    agent_role: str = Field(description="The role of the agent assigned to this task")
    context: Optional[Dict[str, Any]] = Field(default=None, description="Additional context for the task")
    
    model_config = {"extra": "forbid"}

def wait_for_ollama(base_url: str, timeout: int = 30, interval: int = 2) -> bool:
    """Wait for Ollama server to be ready"""
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(f"{base_url}/api/version")
            if response.status_code == 200:
                return True
        except requests.exceptions.RequestException:
            pass
        time.sleep(interval)
    return False

# Get task message from command line argument
task_description = sys.argv[1]

# Setup Ollama configuration
base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
model = os.getenv("OLLAMA_MODEL", "llama2")

# Wait for Ollama to be ready
print(colored("\nChecking Ollama connection...", "yellow"))
if not wait_for_ollama(base_url):
    print(colored("Error: Could not connect to Ollama server", "red"))
    sys.exit(1)

# Initialize Ollama LLM with streaming
llm = Ollama(
    base_url=base_url,
    model=model,
    callbacks=[StreamingStdOutCallbackHandler()],
    temperature=0.5,
    timeout=120
)

# Define agent configurations
agent_configs = [
    AgentConfig(
        role='Software Architect',
        goal='Design and plan software architecture',
        backstory="""Expert software architect with extensive experience in system design.
        Focuses on creating clean, maintainable, and well-structured applications.
        Proficient in various design patterns and best practices."""
    ),
    AgentConfig(
        role='Software Developer',
        goal='Implement high-quality code',
        backstory="""Senior developer with strong coding skills and attention to detail.
        Writes clean, efficient, and well-documented code.
        Expert in implementing robust error handling and testing."""
    ),
    AgentConfig(
        role='Code Reviewer',
        goal='Ensure code quality and best practices',
        backstory="""Experienced code reviewer with a keen eye for detail.
        Ensures code meets quality standards, follows best practices, and is well-tested.
        Expert in identifying potential issues and suggesting improvements."""
    )
]

# Define task configurations
task_configs = [
    TaskConfig(
        description=f"""Design and create a hello.py file that prints 'Hello, World!'.

        Requirements:
        1. Create a file named 'hello.py' in the workspace directory
        2. The file should print 'Hello, World!' when executed
        3. Follow PEP 8 style guidelines
        
        Available tools:
        - write_file: Use this to write the file (parameters: filename, content)
        - read_file: Use this to read files (parameters: filename)
        - list_files: Use this to list workspace files (no parameters)
        
        The workspace directory is: {WORKSPACE_DIR}
        
        Steps:
        1. Use write_file tool to create hello.py with the following content:
           ```python
           print('Hello, World!')
           ```
        2. Use read_file tool to verify the file was created correctly""",
        agent_role='Software Architect'
    ),
    TaskConfig(
        description=f"""Review and verify the hello.py implementation.
        
        Requirements:
        1. Read the hello.py file
        2. Verify it follows PEP 8 guidelines
        3. Confirm it prints 'Hello, World!'
        4. Write a review summary
        
        Available tools:
        - read_file: Use this to read the file (parameters: filename)
        - write_file: Use this to write review (parameters: filename, content)
        
        Steps:
        1. Use read_file tool to read hello.py
        2. Review the implementation
        3. Use write_file tool to write review.txt with your findings""",
        agent_role='Code Reviewer'
    )
]

# Create agents
agents = []
for config in agent_configs:
    agent = Agent(
        role=config.role,
        goal=config.goal,
        backstory=config.backstory,
        verbose=config.verbose,
        allow_delegation=config.allow_delegation,
        llm=llm,
        tools=tools
    )
    agents.append(agent)

# Create tasks
tasks = []
for config in task_configs:
    agent = next((a for a in agents if a.role == config.agent_role), None)
    if agent:
        task = Task(
            description=config.description,
            agent=agent
        )
        tasks.append(task)

# Create and run crew
crew = Crew(
    agents=agents,
    tasks=tasks,
    verbose=2,
    process=Process.sequential
)

# Execute the task
result = crew.kickoff()
print("\nTask execution completed!")
print("Result:", result)

EOF
}

# Cleanup function
cleanup() {
    stop_tunnel
    if [ -f "$OLLAMA_PID_FILE" ]; then
        PID=$(cat "$OLLAMA_PID_FILE")
        if kill -0 "$PID" 2>/dev/null; then
            kill "$PID"
            rm "$OLLAMA_PID_FILE"
            echo "Ollama server stopped"
        fi
    fi
    exit 0
}

# Set up trap for cleanup
trap cleanup EXIT INT TERM

# Load environment variables
load_env

# Command line interface
case "$1" in
    "setup")
        setup_env
        if ! select_ollama_mode; then
            echo "Failed to select Ollama mode"
            exit 1
        fi
        ensure_ollama
        echo "Optimizing Ollama for maximum performance..."
        if [ "$(get_ollama_mode)" = "remote" ] && optimize_ollama; then
            echo "Ollama optimization completed."
            touch "$CONFIG_DIR/.ollama_optimized"
        else
            echo "Warning: Ollama optimization skipped or failed, continuing with default settings."
        fi
        echo "DevTeam environment is ready!"
        ;;
    "mode")
        case "$2" in
            "")
                select_ollama_mode
                ;;
            "local"|"remote")
                select_ollama_mode "$2"
                ;;
            *)
                echo "Usage: ./devteam mode [local|remote]"
                exit 1
                ;;
        esac
        ;;
    "task")
        if [ -z "$2" ]; then
            echo "Usage: ./devteam task 'your task description'"
            exit 1
        fi
        setup_env
        ensure_ollama
        run_team "$2"
        ;;
    *)
        echo "Usage:"
        echo "  ./devteam setup              - Set up the development environment"
        echo "  ./devteam mode [local|remote] - Select Ollama mode (local/remote)"
        echo "  ./devteam task 'description' - Assign a task to the team"
        exit 1
        ;;
esac 